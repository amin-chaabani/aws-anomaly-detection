# ðŸ“š Modeling Phase - Lessons Learned

## ðŸŽ¯ Project: AWS Anomaly Detection System

**Date:** November 4, 2025  
**Phase:** 4 - Modeling  
**Final Model:** One-Class SVM (Optuna 40 trials)

---

## âœ… What Worked Well

### 1. Stratified Data Split
**Problem Solved:** Original temporal split resulted in 0% anomalies in validation set  
**Solution:** Implemented stratified train/val/test split (70/15/15)  
**Result:** Balanced distribution (~26% anomalies across all sets)

### 2. Optuna Hyperparameter Optimization
**Approach:** Bayesian optimization with 40 trials for each model  
**Benefits:**
- Systematic exploration of hyperparameter space
- Reproducible results with random_state=42
- F1-Score as optimization objective

### 3. Multiple Model Evaluation
**Models Tested:**
- Isolation Forest (F1 Test: 0.5000) - Overfitting detected
- One-Class SVM (F1 Test: 0.6250) âœ… Best
- Local Outlier Factor (F1 Test: 0.4286) - Overfitting detected
- Ensemble (F1 Test: 0.5333) - Overfitting detected

### 4. Overfitting Detection
**Metric:** Difference between validation and test F1-scores  
**Threshold:** < 0.10 for good generalization  
**Result:** One-Class SVM showed excellent stability (diff = 0.0066)

---

## âŒ What Didn't Work

### 1. Feature Selection with Random Forest
**Attempted:** Reduce from 104 to 40 features using RF feature importance  
**Problem:** Created overfitting - model adapted too much to validation set  
**Result:**
- Validation F1: 0.7368 âœ…
- Test F1: 0.4706 âŒ (worse than baseline!)
- Difference: 0.2663 (massive overfitting)

**Lesson:** Feature selection should be done on training set only, validated on separate data.

### 2. Excessive Optuna Trials (100 trials)
**Attempted:** Double optimization trials from 40 to 100  
**Problem:** Overfitting to validation set with small dataset (230 samples)  
**Result:** Model memorized validation patterns instead of learning general patterns  

**Lesson:** More trials â‰  better results. With small datasets, moderate trials (40-50) are sufficient.

### 3. Complex Multi-Objective Optimization
**Attempted:** Optimize F1-Score with FPR penalty  
**Problem:** Added complexity without improving real-world performance  
**Result:** Model performed worse on test set despite higher validation scores

**Lesson:** Keep optimization objectives simple and aligned with business metrics.

---

## ðŸ§  Key Insights

### Data Size Matters
- **Dataset:** 230 total samples (161 train, 34 val, 35 test)
- **Finding:** Small datasets require simpler models and fewer optimization trials
- **Action:** Avoided complex ensembles and excessive tuning

### Validation â‰  Test Performance
- **Key Learning:** High validation scores don't guarantee good test performance
- **Best Practice:** Always verify on independent test set before selecting final model
- **Our Approach:** Systematic comparison of val vs test metrics for all models

### Simplicity Wins
- **Observation:** One-Class SVM with standard features outperformed complex approaches
- **Reason:** Simpler models generalize better on small datasets
- **Takeaway:** Start simple, add complexity only if justified by test performance

### Overfitting Indicators
**Red Flags:**
- âœ… Val F1 >> Test F1 (difference > 0.15)
- âœ… Precision = 1.0 on validation (too perfect)
- âœ… Performance drops significantly on test set

**Our Case:**
- IF/LOF/Ensemble: Perfect precision (1.0) on val, dropped on test
- SVM: Balanced performance (0.60-0.71) maintained across val/test

---

## ðŸ“Š Final Model Justification

### Why One-Class SVM?

**1. Best Generalization**
- Validation F1: 0.6316
- Test F1: 0.6250
- Difference: 0.0066 âœ… (excellent stability)

**2. No Overfitting**
- Consistent performance across validation and test
- No signs of memorization
- Robust predictions

**3. Balanced Metrics**
- Precision: 0.7143 (low false positives)
- Recall: 0.5556 (acceptable true positive rate)
- Good trade-off for anomaly detection

**4. Production-Ready**
- Simple sklearn model (easy deployment)
- Fast inference
- All 104 features used (no feature engineering complexity)
- Reproducible with saved config

---

## ðŸŽ“ Best Practices Established

### For Small Datasets (<500 samples):
1. âœ… Use stratified splits to ensure class balance
2. âœ… Limit hyperparameter optimization trials (30-50)
3. âœ… Prefer simpler models over complex ensembles
4. âœ… Always validate on independent test set
5. âœ… Monitor val/test performance difference closely

### For Anomaly Detection:
1. âœ… Optimize for F1-Score (balance of precision/recall)
2. âœ… Accept moderate performance over perfect validation scores
3. âœ… Verify no overfitting before deployment
4. âœ… Document all modeling decisions

### For Production:
1. âœ… Save model, scaler, and feature names
2. âœ… Document hyperparameters and performance
3. âœ… Create JSON config for easy interpretation
4. âœ… Include timestamp and validation metrics

---

## ðŸ“ˆ Performance Comparison

| Model | Val F1 | Test F1 | Difference | Overfitting | Status |
|-------|--------|---------|------------|-------------|--------|
| **SVM (40 trials)** | **0.6316** | **0.6250** | **0.0066** | **No** | **âœ… SELECTED** |
| SVM + FS (100 trials) | 0.7368 | 0.4706 | 0.2663 | Yes | âŒ Rejected |
| Isolation Forest | 0.7143 | 0.5000 | 0.2143 | Yes | âŒ Rejected |
| LOF | 0.6154 | 0.4286 | 0.1868 | Yes | âŒ Rejected |
| Ensemble | 0.7143 | 0.5333 | 0.1810 | Yes | âŒ Rejected |

---

## ðŸš€ Recommendations for Future Work

### Immediate Next Steps:
1. Proceed to Phase 5 (Evaluation) with selected model
2. Perform detailed error analysis
3. Calculate business impact metrics
4. Create deployment documentation

### Future Improvements (if more data available):
1. Collect more samples to increase dataset size
2. Experiment with deep learning approaches (LSTM Autoencoder)
3. Implement real-time anomaly detection pipeline
4. Add explainability features (SHAP, LIME)

### Model Monitoring in Production:
1. Track F1-Score, Precision, Recall over time
2. Monitor for data drift
3. Retrain monthly with new data
4. Implement automated overfitting checks

---

## ðŸ“ Saved Artifacts

### Models:
- `one_class_svm_final.pkl` - Final trained model
- `scaler.pkl` - StandardScaler for features
- `feature_names.pkl` - List of 104 feature names

### Configuration:
- `final_model_config.pkl` - Complete model configuration
- `final_model_config.json` - Human-readable config
- `final_model_selection.csv` - Model comparison results

### Reports:
- `model_results.csv` - All model metrics
- `04_*.png` - Comparison visualizations
- `04_*.html` - Interactive Optuna plots

---

## ðŸŽ¯ Conclusion

**Key Success:** Selected a stable, production-ready model that generalizes well to unseen data.

**Critical Decision:** Rejected models with higher validation scores but poor test performance, avoiding the overfitting trap.

**Impact:** Confident in model deployment with F1-Score of 0.625 and excellent generalization (diff < 0.01).

**Next Phase:** Comprehensive evaluation and business impact assessment in Phase 5.

---

**Document Owner:** Yassmine  
**Project:** AWS Anomaly Detection System  
**Last Updated:** November 4, 2025
