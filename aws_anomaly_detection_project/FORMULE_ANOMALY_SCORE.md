# ðŸ§® FORMULE MATHÃ‰MATIQUE DU SCORE D'ANOMALIE

## ðŸ“ LA FORMULE EXACTE

### **Pour le One-Class SVM (ton modÃ¨le)**

Le score d'anomalie est calculÃ© par la fonction **`decision_function()`** du SVM :

```
score(x) = w^T Â· Ï†(x) - Ï
```

**OÃ¹ :**
- `x` = Tes donnÃ©es (les 350 features aprÃ¨s feature engineering)
- `w` = Vecteur de poids du modÃ¨le (normal Ã  l'hyperplan)
- `Ï†(x)` = Transformation kernel (RBF dans notre cas)
- `Ï` (rho) = DÃ©calage de l'hyperplan (offset)
- `^T` = TransposÃ©e du vecteur

---

## ðŸŽ¯ EXPLICATION SIMPLE

### **Version simplifiÃ©e :**
```
Score = Distance signÃ©e de x Ã  l'hyperplan de dÃ©cision

Si Score > 0  â†’ x est du cÃ´tÃ© "NORMAL" âœ…
Si Score < 0  â†’ x est du cÃ´tÃ© "ANOMALIE" âŒ
Si Score = 0  â†’ x est exactement sur la frontiÃ¨re
```

---

## ðŸ”¬ FORMULE DÃ‰TAILLÃ‰E AVEC RBF KERNEL

Notre modÃ¨le utilise un **kernel RBF (Radial Basis Function)**, donc :

### **Ã‰tape 1 : Kernel RBF**
```
Ï†(x) = Kernel RBF

K(x, x_i) = exp(-Î³ Â· ||x - x_i||Â²)
```

**OÃ¹ :**
- `x` = Nouvelle observation (tes donnÃ©es)
- `x_i` = Vecteurs de support du modÃ¨le (exemples d'entraÃ®nement)
- `Î³` (gamma) = ParamÃ¨tre du kernel (contrÃ´le la "portÃ©e")
- `||x - x_i||Â²` = Distance euclidienne au carrÃ©

### **Ã‰tape 2 : Calcul du score**
```
score(x) = Î£ Î±_i Â· K(x, x_i) - Ï
           i=1 Ã  n_support_vectors
```

**OÃ¹ :**
- `Î±_i` = Coefficients des vecteurs de support (appris pendant l'entraÃ®nement)
- `K(x, x_i)` = SimilaritÃ© kernel entre x et chaque vecteur de support
- `Ï` = Seuil de dÃ©cision (offset)
- La somme est sur tous les **vecteurs de support** du modÃ¨le

---

## ðŸ’» CALCUL DANS LE CODE

### **En Python avec scikit-learn :**

```python
from sklearn.svm import OneClassSVM
import numpy as np

# 1. ModÃ¨le entraÃ®nÃ© (dÃ©jÃ  fait)
model = OneClassSVM(kernel='rbf', gamma=0.01, nu=0.1)
model.fit(X_train)  # X_train = donnÃ©es d'entraÃ®nement normales

# 2. PrÃ©diction binaire
prediction = model.predict(X_new)
# Retourne : +1 (normal) ou -1 (anomalie)

# 3. CALCUL DU SCORE D'ANOMALIE
anomaly_score = model.decision_function(X_new)
# Retourne : score rÃ©el (ex: -0.75)
```

### **Ce qui se passe en interne :**

```python
# Pseudo-code simplifiÃ©
def decision_function(X_new):
    score = 0
    
    # Pour chaque vecteur de support
    for i in range(n_support_vectors):
        x_i = support_vectors[i]
        alpha_i = dual_coef[i]
        
        # Calcul du kernel RBF
        distance_squared = np.sum((X_new - x_i) ** 2)
        kernel_value = np.exp(-gamma * distance_squared)
        
        # Accumulation
        score += alpha_i * kernel_value
    
    # Soustraction du seuil
    score -= rho
    
    return score
```

---

## ðŸ§ª EXEMPLE DE CALCUL AVEC TES DONNÃ‰ES

### **Tes donnÃ©es :**
```json
{
  "CPU": 0.95,
  "MÃ©moire": 0.92,
  "Pods": 0.88
}
```

### **Ã‰tape 1 : Feature Engineering**
```python
# 3 mÃ©triques â†’ 350 features
features = [
    0.95,  # cpu_ratio
    0.92,  # mem_ratio
    0.88,  # pod_ratio
    0.85,  # cpu_mean_3h
    0.08,  # cpu_std_6h
    ...    # 345 autres features
]  # Total : 350 valeurs
```

### **Ã‰tape 2 : Normalisation**
```python
# Avec le scaler entraÃ®nÃ©
X_scaled = (features - mean) / std
# Exemple : X_scaled = [2.5, 2.1, 1.8, 1.9, 0.3, ...]
```

### **Ã‰tape 3 : Calcul du kernel avec chaque vecteur de support**

Supposons que le modÃ¨le a **500 vecteurs de support** :

```python
gamma = 0.01  # ParamÃ¨tre du modÃ¨le

score = 0
for i in range(500):
    # Distance euclidienne au carrÃ©
    distanceÂ² = Î£(X_scaled[j] - support_vector_i[j])Â²
    # Par exemple : distanceÂ² = 25.3
    
    # Kernel RBF
    K = exp(-0.01 Ã— 25.3) = exp(-0.253) = 0.776
    
    # Coefficient alpha
    alpha_i = 0.0015  # (exemple)
    
    # Accumulation
    score += 0.0015 Ã— 0.776 = 0.001164
```

AprÃ¨s avoir sommÃ© les 500 vecteurs de support :
```python
total_sum = 0.523  # (exemple)
```

### **Ã‰tape 4 : Soustraction du seuil rho**
```python
rho = 1.273  # Valeur apprise pendant l'entraÃ®nement

score_final = 0.523 - 1.273 = -0.75
```

**RÃ©sultat : `-0.75` ðŸŽ¯**

---

## ðŸ“Š PARAMÃˆTRES DU MODÃˆLE

Ces valeurs sont **apprises automatiquement** pendant l'entraÃ®nement :

| ParamÃ¨tre | Description | Valeur typique |
|-----------|-------------|----------------|
| **Î³ (gamma)** | PortÃ©e du kernel RBF | 0.001 - 0.1 |
| **Î½ (nu)** | Fraction d'anomalies attendues | 0.05 - 0.15 |
| **Ï (rho)** | Seuil de dÃ©cision | Appris automatiquement |
| **Î±_i** | Coefficients des vecteurs support | Appris automatiquement |
| **Support vectors** | Exemples clÃ©s de l'entraÃ®nement | 100 - 1000+ |

### **Dans ton modÃ¨le :**
```python
# ParamÃ¨tres dÃ©finis
gamma = 'scale'  # Automatique : 1 / (n_features Ã— variance)
nu = 0.1         # 10% d'anomalies attendues

# ParamÃ¨tres appris
rho = model.offset_[0]              # Ex: 1.273
dual_coef = model.dual_coef_        # Ex: array de 500 valeurs
support_vectors = model.support_vectors_  # Ex: 500 vecteurs de 350 features
```

---

## ðŸ”¢ FORMULE COMPLÃˆTE DÃ‰VELOPPÃ‰E

### **Version mathÃ©matique complÃ¨te :**

$$
f(x) = \sum_{i=1}^{n_{SV}} \alpha_i \cdot \exp\left(-\gamma \sum_{j=1}^{350} (x_j - x_{i,j})^2\right) - \rho
$$

**OÃ¹ :**
- $f(x)$ = Score d'anomalie (ton -0.75)
- $n_{SV}$ = Nombre de vecteurs de support (ex: 500)
- $\alpha_i$ = Coefficient du i-Ã¨me vecteur de support
- $\gamma$ = ParamÃ¨tre du kernel RBF
- $x_j$ = j-Ã¨me feature de ta nouvelle observation (350 features)
- $x_{i,j}$ = j-Ã¨me feature du i-Ã¨me vecteur de support
- $\rho$ = Seuil de dÃ©cision (offset)

### **InterprÃ©tation :**
```
Si f(x) > 0  â†’  NORMAL   (prÃ©diction = +1)
Si f(x) < 0  â†’  ANOMALIE (prÃ©diction = -1)
```

---

## ðŸŽ¨ VISUALISATION DU CALCUL

```
Tes 350 features (aprÃ¨s normalisation)
        â”‚
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pour chaque vecteur de support:    â”‚
â”‚                                       â”‚
â”‚   1. Calcule distance euclidienne    â”‚
â”‚   2. Applique kernel RBF             â”‚
â”‚   3. Multiplie par coefficient Î±_i   â”‚
â”‚   4. Additionne                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â†“
    Somme totale = 0.523
        â”‚
        â†“
    Soustrais Ï (rho) = 1.273
        â”‚
        â†“
    Score final = -0.75 âœ…
        â”‚
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Score < 0 â†’ ANOMALIE DÃ‰TECTÃ‰E! ðŸ”´   â”‚
â”‚  Magnitude Ã©levÃ©e â†’ CRITIQUE          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ§  POURQUOI CETTE FORMULE EST INTELLIGENTE ?

### **1. Le kernel RBF capture la similaritÃ©**
```
Distance petite â†’ Kernel proche de 1 â†’ Similaire âœ…
Distance grande â†’ Kernel proche de 0 â†’ DiffÃ©rent âŒ
```

### **2. Les vecteurs de support sont les "exemples clÃ©s"**
Ce sont les observations d'entraÃ®nement les plus **reprÃ©sentatives** du comportement normal.

### **3. Les coefficients Î±_i pondÃ¨rent l'importance**
Certains vecteurs de support comptent plus que d'autres.

### **4. Le seuil Ï dÃ©finit la frontiÃ¨re**
C'est la "barre" entre normal et anormal, optimisÃ©e pendant l'entraÃ®nement.

---

## ðŸ“ EXEMPLE NUMÃ‰RIQUE COMPLET

### **Configuration du modÃ¨le :**
```
Î³ (gamma) = 0.01
Î½ (nu) = 0.1
Ï (rho) = 1.273
Nombre de support vectors = 500
```

### **Tes features normalisÃ©es (extrait) :**
```
X_scaled = [2.5, 2.1, 1.8, 1.9, 0.3, ..., 1.2]  # 350 valeurs
```

### **Calcul pour le premier vecteur de support :**
```
SVâ‚ = [0.1, 0.2, 0.15, 0.18, 0.05, ..., 0.12]
Î±â‚ = 0.0015

DistanceÂ² = (2.5-0.1)Â² + (2.1-0.2)Â² + ... + (1.2-0.12)Â²
         = 5.76 + 3.61 + ... + 1.17
         = 25.3

Kernel = exp(-0.01 Ã— 25.3) = exp(-0.253) = 0.776

Contribution = 0.0015 Ã— 0.776 = 0.001164
```

### **RÃ©pÃ©ter pour les 500 vecteurs de support :**
```
Somme totale = 0.523
```

### **Score final :**
```
score = 0.523 - 1.273 = -0.75 âœ…
```

---

## ðŸ” VÃ‰RIFICATION DANS LE CODE

### **AccÃ©der aux paramÃ¨tres du modÃ¨le :**

```python
import pickle

# Charger le modÃ¨le
with open('models/one_class_svm_final.pkl', 'rb') as f:
    model = pickle.load(f)

# Afficher les paramÃ¨tres
print("ParamÃ¨tres du modÃ¨le:")
print(f"Gamma: {model.gamma}")
print(f"Nu: {model.nu}")
print(f"Rho (offset): {model.offset_[0]}")
print(f"Nombre de support vectors: {len(model.support_vectors_)}")
print(f"Shape support vectors: {model.support_vectors_.shape}")
print(f"Coefficients duaux: {model.dual_coef_.shape}")

# Exemple de calcul manuel
X_new = [[...]]  # Tes 350 features normalisÃ©es

# MÃ©thode 1 : Directe
score = model.decision_function(X_new)[0]
print(f"\nScore d'anomalie: {score:.4f}")

# MÃ©thode 2 : Calcul manuel (pour comprendre)
from scipy.spatial.distance import cdist

# Distance aux support vectors
distances = cdist(X_new, model.support_vectors_, metric='euclidean')
distances_squared = distances ** 2

# Kernel RBF
kernel_values = np.exp(-model.gamma * distances_squared)

# Score
score_manual = (kernel_values @ model.dual_coef_.T).ravel() - model.offset_
print(f"Score calculÃ© manuellement: {score_manual[0]:.4f}")
```

---

## ðŸ“š RÃ‰SUMÃ‰ DE LA FORMULE

### **Version ultra-simplifiÃ©e :**
```
Score = SimilaritÃ©_aux_exemples_normaux - Seuil

Si Score > 0 â†’ Ressemble aux exemples normaux âœ…
Si Score < 0 â†’ Ne ressemble PAS aux normaux âŒ
```

### **Version mathÃ©matique :**
```
score(x) = Î£[Î±áµ¢ Â· K(x, xáµ¢)] - Ï

Avec K(x, xáµ¢) = exp(-Î³||x - xáµ¢||Â²)
```

### **Pour ton cas (-0.75) :**
```
Tes donnÃ©es (95%, 92%, 88%)
    â†“ Feature engineering
350 features
    â†“ Normalisation
Features scaled
    â†“ Kernel RBF avec 500 support vectors
SimilaritÃ© totale = 0.523
    â†“ Soustraire seuil Ï = 1.273
Score = -0.75
    â†“
ANOMALIE CRITIQUE ðŸ”´
```

---

## ðŸŽ¯ CONCLUSION

**OUI, il existe une formule mathÃ©matique prÃ©cise !**

C'est une combinaison de :
1. **Distances** (entre tes donnÃ©es et les exemples d'entraÃ®nement)
2. **Kernel RBF** (transformation non-linÃ©aire)
3. **Coefficients appris** (Î±_i et Ï)

Le modÃ¨le **ne fait PAS de magie** - c'est du **calcul mathÃ©matique pur** basÃ© sur des formules bien dÃ©finies ! ðŸ§®

---

**Tu comprends maintenant la formule complÃ¨te ! ðŸŽ‰**
