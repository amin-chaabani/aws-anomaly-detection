{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dff953c",
   "metadata": {},
   "source": [
    "# Phase 5: Evaluation\n",
    "\n",
    "## CRISP-DM - Evaluation Phase\n",
    "\n",
    "**Objective:** Comprehensive performance evaluation of all trained models on the test set.\n",
    "\n",
    "**Key Activities:**\n",
    "1. Test set evaluation (final model performance)\n",
    "2. ROC and Precision-Recall curves\n",
    "3. Confusion matrices and classification reports\n",
    "4. Model comparison visualizations (radar plots, bar charts)\n",
    "5. Feature importance analysis\n",
    "6. Error analysis (false positives/negatives investigation)\n",
    "7. Detection latency measurement\n",
    "8. Final model selection and justification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6410278f",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715f3021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, accuracy_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec3408",
   "metadata": {},
   "source": [
    "## 2. Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf67b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded final model: One-Class SVM\n",
      "‚úÖ Loaded model configuration\n",
      "\n",
      "‚úÖ Model and data loaded successfully\n",
      "Test set: 35 samples, 104 features\n",
      "Test outliers: 32 / 35 (91.4%)\n",
      "\n",
      "Model: One-Class SVM (Tuned)\n",
      "Hyperparameters: {'kernel': 'rbf', 'nu': 0.18, 'gamma': 'scale'}\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path('../data/processed')\n",
    "MODELS_DIR = Path('../models')\n",
    "REPORTS_DIR = Path('../reports/figures')\n",
    "\n",
    "# Load test data\n",
    "X_test = pd.read_csv(DATA_DIR / 'X_test.csv', index_col=0, parse_dates=True)\n",
    "y_test = pd.read_csv(DATA_DIR / 'y_test.csv', index_col=0, parse_dates=True).squeeze()\n",
    "\n",
    "# Load validation data (for comparison)\n",
    "X_val = pd.read_csv(DATA_DIR / 'X_val.csv', index_col=0, parse_dates=True)\n",
    "y_val = pd.read_csv(DATA_DIR / 'y_val.csv', index_col=0, parse_dates=True).squeeze()\n",
    "\n",
    "# Load feature names\n",
    "with open(MODELS_DIR / 'feature_names.pkl', 'rb') as f:\n",
    "    feature_names = pickle.load(f)\n",
    "\n",
    "# Load FINAL model (One-Class SVM)\n",
    "with open(MODELS_DIR / 'one_class_svm_final.pkl', 'rb') as f:\n",
    "    final_model = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded final model: One-Class SVM\")\n",
    "\n",
    "# Load model configuration\n",
    "with open(MODELS_DIR / 'final_model_config.pkl', 'rb') as f:\n",
    "    model_config = pickle.load(f)\n",
    "print(f\"‚úÖ Loaded model configuration\")\n",
    "\n",
    "print(f\"\\n‚úÖ Model and data loaded successfully\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
    "print(f\"Test outliers: {y_test.sum()} / {len(y_test)} ({y_test.mean()*100:.1f}%)\")\n",
    "print(f\"\\nModel: {model_config['model_name']}\")\n",
    "print(f\"Hyperparameters: {model_config['sklearn_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1bc318",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è ATTENTION: Probl√®me de Donn√©es D√©tect√©\n",
    "\n",
    "**Observation:** Le test set charg√© contient 91.4% d'anomalies, ce qui correspond √† l'ancien split temporel NON stratifi√©!\n",
    "\n",
    "**Solution:** Nous devons recharger les donn√©es depuis le notebook `04_modeling.ipynb` o√π nous avons cr√©√© le split stratifi√© correct (70/15/15 avec ~26% d'anomalies dans chaque set).\n",
    "\n",
    "**Action:** Recr√©ons le split stratifi√© ici pour l'√©valuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f96cbf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Recr√©ation du split stratifi√© correct pour l'√©valuation...\n",
      "Dataset complet: 230 samples, 60 anomalies (26.09%)\n",
      "\n",
      "‚úÖ Split stratifi√© cr√©√©:\n",
      "Train: 161 samples (42 anomalies, 26.09%)\n",
      "Val:   34 samples (9 anomalies, 26.47%)\n",
      "Test:  35 samples (9 anomalies, 25.71%)\n",
      "\n",
      "üéØ Utilisation des donn√©es TEST pour l'√©valuation finale\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"üîÑ Recr√©ation du split stratifi√© correct pour l'√©valuation...\")\n",
    "\n",
    "# Charger toutes les donn√©es\n",
    "X_train_orig = pd.read_csv(DATA_DIR / 'X_train.csv', index_col=0, parse_dates=True)\n",
    "X_val_orig = pd.read_csv(DATA_DIR / 'X_val.csv', index_col=0, parse_dates=True)\n",
    "X_test_orig = pd.read_csv(DATA_DIR / 'X_test.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "y_train_orig = pd.read_csv(DATA_DIR / 'y_train.csv', index_col=0, parse_dates=True).squeeze()\n",
    "y_val_orig = pd.read_csv(DATA_DIR / 'y_val.csv', index_col=0, parse_dates=True).squeeze()\n",
    "y_test_orig = pd.read_csv(DATA_DIR / 'y_test.csv', index_col=0, parse_dates=True).squeeze()\n",
    "\n",
    "# Combiner tout\n",
    "X_all = pd.concat([X_train_orig, X_val_orig, X_test_orig])\n",
    "y_all = pd.concat([y_train_orig, y_val_orig, y_test_orig])\n",
    "\n",
    "print(f\"Dataset complet: {X_all.shape[0]} samples, {y_all.sum()} anomalies ({y_all.mean()*100:.2f}%)\")\n",
    "\n",
    "# Split stratifi√© 70/15/15\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_all, y_all, \n",
    "    test_size=0.30,\n",
    "    stratify=y_all,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.50,\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Split stratifi√© cr√©√©:\")\n",
    "print(f\"Train: {X_train.shape[0]} samples ({y_train.sum()} anomalies, {y_train.mean()*100:.2f}%)\")\n",
    "print(f\"Val:   {X_val.shape[0]} samples ({y_val.sum()} anomalies, {y_val.mean()*100:.2f}%)\")\n",
    "print(f\"Test:  {X_test.shape[0]} samples ({y_test.sum()} anomalies, {y_test.mean()*100:.2f}%)\")\n",
    "print(\"\\nüéØ Utilisation des donn√©es TEST pour l'√©valuation finale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eb471c",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dfa6ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_comprehensive(model, X, y, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with timing\n",
    "    \"\"\"\n",
    "    # Measure prediction time\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X)\n",
    "    predict_time = time.time() - start_time\n",
    "    \n",
    "    # Convert to binary\n",
    "    y_pred_binary = (y_pred == -1).astype(int)\n",
    "    \n",
    "    # Metrics\n",
    "    precision = precision_score(y, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y, y_pred_binary, zero_division=0)\n",
    "    f1 = f1_score(y, y_pred_binary, zero_division=0)\n",
    "    accuracy = accuracy_score(y, y_pred_binary)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y, y_pred_binary)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Rates\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # Latency (per sample)\n",
    "    latency_per_sample = (predict_time / len(X)) * 1000  # milliseconds\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'FPR': fpr,\n",
    "        'FNR': fnr,\n",
    "        'TPR': tpr,\n",
    "        'TNR': tnr,\n",
    "        'TP': int(tp),\n",
    "        'FP': int(fp),\n",
    "        'TN': int(tn),\n",
    "        'FN': int(fn),\n",
    "        'Predict_Time': predict_time,\n",
    "        'Latency_ms': latency_per_sample,\n",
    "        'Predictions': y_pred_binary\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29703906",
   "metadata": {},
   "source": [
    "## 4. Test Set Evaluation\n",
    "\n",
    "### 4.1 Evaluate All Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad0d4395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Evaluating FINAL MODEL on TEST set...\n",
      "================================================================================\n",
      "\n",
      "Evaluating One-Class SVM (Tuned)...\n",
      "\n",
      "================================================================================\n",
      "  üìä TEST SET RESULTS - FINAL MODEL\n",
      "================================================================================\n",
      "  Model: One-Class SVM (Tuned)\n",
      "  Precision: 0.7143\n",
      "  Recall: 0.5556\n",
      "  F1-Score: 0.6250\n",
      "  Accuracy: 0.8286\n",
      "  FPR: 0.0769\n",
      "  FNR: 0.4444\n",
      "\n",
      "  Confusion Matrix:\n",
      "    TP:   5  |  FP:   2\n",
      "    FN:   4  |  TN:  24\n",
      "\n",
      "  Performance:\n",
      "    Prediction Time: 0.0049s\n",
      "    Latency: 0.14 ms/sample\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Evaluation complete!\n",
      "Detected anomalies: 7 / 35 (20.0%)\n",
      "True anomalies: 9 / 35 (25.7%)\n"
     ]
    }
   ],
   "source": [
    "print(\"üéØ Evaluating FINAL MODEL on TEST set...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Evaluate the final model\n",
    "print(f\"\\nEvaluating {model_config['model_name']}...\")\n",
    "\n",
    "test_metrics = evaluate_model_comprehensive(final_model, X_test, y_test, model_config['model_name'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  üìä TEST SET RESULTS - FINAL MODEL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Model: {test_metrics['Model']}\")\n",
    "print(f\"  Precision: {test_metrics['Precision']:.4f}\")\n",
    "print(f\"  Recall: {test_metrics['Recall']:.4f}\")\n",
    "print(f\"  F1-Score: {test_metrics['F1-Score']:.4f}\")\n",
    "print(f\"  Accuracy: {test_metrics['Accuracy']:.4f}\")\n",
    "print(f\"  FPR: {test_metrics['FPR']:.4f}\")\n",
    "print(f\"  FNR: {test_metrics['FNR']:.4f}\")\n",
    "print(\"\\n  Confusion Matrix:\")\n",
    "print(f\"    TP: {test_metrics['TP']:>3}  |  FP: {test_metrics['FP']:>3}\")\n",
    "print(f\"    FN: {test_metrics['FN']:>3}  |  TN: {test_metrics['TN']:>3}\")\n",
    "print(f\"\\n  Performance:\")\n",
    "print(f\"    Prediction Time: {test_metrics['Predict_Time']:.4f}s\")\n",
    "print(f\"    Latency: {test_metrics['Latency_ms']:.2f} ms/sample\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Store predictions for later analysis\n",
    "y_test_pred = test_metrics['Predictions']\n",
    "y_test_pred_labels = (y_test_pred == 1)  # True = Anomaly\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation complete!\")\n",
    "print(f\"Detected anomalies: {y_test_pred.sum()} / {len(y_test)} ({y_test_pred.mean()*100:.1f}%)\")\n",
    "print(f\"True anomalies: {y_test.sum()} / {len(y_test)} ({y_test.mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0251c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Performance Comparison Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics_to_plot = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'FPR', 'Latency_ms']\n",
    "colors = ['#3498db']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    values = test_results_df[metric]\n",
    "    bars = ax.bar(range(len(test_results_df)), values, color=colors[0])\n",
    "    \n",
    "    ax.set_title(f'{metric}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(range(len(test_results_df)))\n",
    "    ax.set_xticklabels(test_results_df['Model'], rotation=45, ha='right', fontsize=9)\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(values):\n",
    "        ax.text(i, v + 0.01 * max(values) if max(values) > 0 else 0.01, \n",
    "                f'{v:.3f}' if metric != 'Latency_ms' else f'{v:.2f}',\n",
    "                ha='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    bars[0].set_color('#27ae60')\n",
    "    bars[0].set_edgecolor('black')\n",
    "    bars[0].set_linewidth(2)\n",
    "\n",
    "plt.suptitle('Test Set Performance - Final Model', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / '05_test_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Saved performance visualization to {REPORTS_DIR / '05_test_performance_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb8e982",
   "metadata": {},
   "source": [
    "### 4.2 Performance Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e572ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics_to_plot = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'FPR', 'Latency_ms']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    values = test_results_df[metric]\n",
    "    bars = ax.bar(range(len(test_results_df)), values, color=colors)\n",
    "    \n",
    "    ax.set_title(f'{metric}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(range(len(test_results_df)))\n",
    "    ax.set_xticklabels(test_results_df['Model'], rotation=45, ha='right', fontsize=9)\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(values):\n",
    "        ax.text(i, v + 0.01 * max(values), f'{v:.3f}' if metric != 'Latency_ms' else f'{v:.2f}',\n",
    "                ha='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Highlight best\n",
    "    if metric not in ['FPR', 'Latency_ms']:  # Lower is better for these\n",
    "        best = values.idxmax()\n",
    "    else:\n",
    "        best = values.idxmin()\n",
    "    bars[best].set_color('#27ae60')\n",
    "    bars[best].set_edgecolor('black')\n",
    "    bars[best].set_linewidth(2)\n",
    "\n",
    "plt.suptitle('Test Set Performance Comparison', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / '05_test_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Saved performance comparison to {REPORTS_DIR / '05_test_performance_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e690b32",
   "metadata": {},
   "source": [
    "### 4.3 Radar Plot (Model Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radar plot for model comparison\n",
    "categories = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'TNR (Specificity)']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for idx, row in test_results_df.iterrows():\n",
    "    values = [\n",
    "        row['Precision'],\n",
    "        row['Recall'],\n",
    "        row['F1-Score'],\n",
    "        row['Accuracy'],\n",
    "        row['TNR']\n",
    "    ]\n",
    "    \n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=values,\n",
    "        theta=categories,\n",
    "        fill='toself',\n",
    "        name=row['Model']\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(visible=True, range=[0, 1])\n",
    "    ),\n",
    "    title=\"Model Performance Radar Chart (Test Set)\",\n",
    "    showlegend=True,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.write_html(REPORTS_DIR / '05_radar_chart.html')\n",
    "fig.show()\n",
    "\n",
    "print(f\"‚úÖ Saved radar chart to {REPORTS_DIR / '05_radar_chart.html'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34324155",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b21d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, result in enumerate(test_results_df.iterrows()):\n",
    "    _, row = result\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = np.array([[row['TN'], row['FP']],\n",
    "                   [row['FN'], row['TP']]])\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n",
    "                xticklabels=['Normal', 'Anomaly'],\n",
    "                yticklabels=['Normal', 'Anomaly'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    ax.set_title(f\"{row['Model']}\\nF1={row['F1-Score']:.3f}\", fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('True Label', fontsize=10)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=10)\n",
    "    \n",
    "    # Add percentages\n",
    "    total = cm.sum()\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            percentage = cm[i, j] / total * 100\n",
    "            ax.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', \n",
    "                   ha='center', va='center', fontsize=9, color='gray')\n",
    "\n",
    "plt.suptitle('Confusion Matrices - Test Set', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / '05_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Saved confusion matrices to {REPORTS_DIR / '05_confusion_matrices.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22261637",
   "metadata": {},
   "source": [
    "## 6. Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e479166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed classification reports\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"  DETAILED CLASSIFICATION REPORTS (Test Set)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx, result in enumerate(test_results_df.iterrows()):\n",
    "    _, row = result\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  {row['Model']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(\n",
    "        y_test, \n",
    "        row['Predictions'],\n",
    "        target_names=['Normal', 'Anomaly'],\n",
    "        digits=4\n",
    "    )\n",
    "    \n",
    "    print(report)\n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  - False Positive Rate: {row['FPR']:.4f}\")\n",
    "    print(f\"  - False Negative Rate: {row['FNR']:.4f}\")\n",
    "    print(f\"  - True Negative Rate (Specificity): {row['TNR']:.4f}\")\n",
    "    print(f\"  - True Positive Rate (Sensitivity): {row['TPR']:.4f}\")\n",
    "    print(f\"\\nLatency:\")\n",
    "    print(f\"  - Per sample: {row['Latency_ms']:.3f} ms\")\n",
    "    print(f\"  - Total ({len(X_test)} samples): {row['Predict_Time']:.3f} seconds\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aceb1b4",
   "metadata": {},
   "source": [
    "## 7. Model Comparison: Validation vs Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7142a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set for comparison\n",
    "print(\"Evaluating models on VALIDATION set for comparison...\\n\")\n",
    "\n",
    "val_results = []\n",
    "for model_key, model in models.items():\n",
    "    display_name = model_names_display[model_key]\n",
    "    metrics = evaluate_model_comprehensive(model, X_val, y_val, display_name)\n",
    "    val_results.append(metrics)\n",
    "\n",
    "val_results_df = pd.DataFrame(val_results)\n",
    "\n",
    "# Compare validation vs test\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': test_results_df['Model'],\n",
    "    'Val_F1': val_results_df['F1-Score'],\n",
    "    'Test_F1': test_results_df['F1-Score'],\n",
    "    'F1_Diff': test_results_df['F1-Score'] - val_results_df['F1-Score'],\n",
    "    'Val_Precision': val_results_df['Precision'],\n",
    "    'Test_Precision': test_results_df['Precision'],\n",
    "    'Val_Recall': val_results_df['Recall'],\n",
    "    'Test_Recall': test_results_df['Recall']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  VALIDATION vs TEST COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for overfitting/underfitting\n",
    "print(\"\\nGeneralization Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "for idx, row in comparison.iterrows():\n",
    "    if abs(row['F1_Diff']) < 0.05:\n",
    "        status = \"‚úÖ Good generalization\"\n",
    "    elif row['F1_Diff'] < -0.05:\n",
    "        status = \"‚ö†Ô∏è Possible overfitting (test worse than val)\"\n",
    "    else:\n",
    "        status = \"‚úÖ Better on test set\"\n",
    "    \n",
    "    print(f\"{row['Model']}: {status} (Œî F1 = {row['F1_Diff']:+.4f})\")\n",
    "\n",
    "# Save comparison\n",
    "comparison.to_csv(REPORTS_DIR.parent / 'val_vs_test_comparison.csv', index=False)\n",
    "print(f\"\\n‚úÖ Saved comparison to {REPORTS_DIR.parent / 'val_vs_test_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3966c50e",
   "metadata": {},
   "source": [
    "## 8. Success Criteria Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business success criteria from Phase 1\n",
    "criteria = {\n",
    "    'Precision': {'target': 0.85, 'unit': ''},\n",
    "    'Recall': {'target': 0.80, 'unit': ''},\n",
    "    'F1-Score': {'target': 0.82, 'unit': ''},\n",
    "    'FPR': {'target': 0.05, 'unit': '', 'lower_is_better': True},\n",
    "    'Latency_ms': {'target': 100, 'unit': 'ms', 'lower_is_better': True}\n",
    "}\n",
    "\n",
    "# Evaluate best model against criteria\n",
    "best_model_results = test_results_df.loc[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"  SUCCESS CRITERIA EVALUATION: {best_model_results['Model']}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_met = True\n",
    "for metric, specs in criteria.items():\n",
    "    actual = best_model_results[metric]\n",
    "    target = specs['target']\n",
    "    unit = specs['unit']\n",
    "    lower_is_better = specs.get('lower_is_better', False)\n",
    "    \n",
    "    if lower_is_better:\n",
    "        met = actual <= target\n",
    "        symbol = \"‚â§\"\n",
    "    else:\n",
    "        met = actual >= target\n",
    "        symbol = \"‚â•\"\n",
    "    \n",
    "    status = \"‚úÖ MET\" if met else \"‚ùå NOT MET\"\n",
    "    all_met = all_met and met\n",
    "    \n",
    "    print(f\"{metric:.<30} Target: {symbol} {target}{unit}  |  Actual: {actual:.4f}{unit}  |  {status}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "if all_met:\n",
    "    print(\"\\nüéâ ALL SUCCESS CRITERIA MET! Model ready for production.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some criteria not met. Consider further tuning or adjusting requirements.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5373143",
   "metadata": {},
   "source": [
    "## 9. Error Analysis\n",
    "\n",
    "### 9.1 Analyze False Positives and False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68273f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on best model for error analysis\n",
    "best_model_key = list(models.keys())[best_idx]\n",
    "best_model = models[best_model_key]\n",
    "best_predictions = best_model_results['Predictions']\n",
    "\n",
    "# Identify error cases\n",
    "false_positives = (best_predictions == 1) & (y_test == 0)\n",
    "false_negatives = (best_predictions == 0) & (y_test == 1)\n",
    "true_positives = (best_predictions == 1) & (y_test == 1)\n",
    "true_negatives = (best_predictions == 0) & (y_test == 0)\n",
    "\n",
    "print(f\"Error Analysis: {best_model_results['Model']}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"False Positives: {false_positives.sum()} samples\")\n",
    "print(f\"False Negatives: {false_negatives.sum()} samples\")\n",
    "print(f\"True Positives: {true_positives.sum()} samples\")\n",
    "print(f\"True Negatives: {true_negatives.sum()} samples\")\n",
    "\n",
    "# Analyze false positives\n",
    "if false_positives.sum() > 0:\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"False Positive Timestamps (first 10):\")\n",
    "    fp_times = X_test[false_positives].index[:10]\n",
    "    for i, ts in enumerate(fp_times, 1):\n",
    "        print(f\"  {i}. {ts}\")\n",
    "\n",
    "# Analyze false negatives\n",
    "if false_negatives.sum() > 0:\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"False Negative Timestamps (first 10):\")\n",
    "    fn_times = X_test[false_negatives].index[:10]\n",
    "    for i, ts in enumerate(fn_times, 1):\n",
    "        print(f\"  {i}. {ts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93cdfb",
   "metadata": {},
   "source": [
    "### 9.2 Visualize Errors Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bde79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create error visualization\n",
    "error_df = pd.DataFrame({\n",
    "    'timestamp': X_test.index,\n",
    "    'true_label': y_test.values,\n",
    "    'predicted': best_predictions,\n",
    "    'error_type': 'Correct'\n",
    "})\n",
    "\n",
    "error_df.loc[false_positives, 'error_type'] = 'False Positive'\n",
    "error_df.loc[false_negatives, 'error_type'] = 'False Negative'\n",
    "error_df.loc[true_positives, 'error_type'] = 'True Positive'\n",
    "\n",
    "# Plot\n",
    "fig = px.scatter(\n",
    "    error_df,\n",
    "    x='timestamp',\n",
    "    y='true_label',\n",
    "    color='error_type',\n",
    "    title=f'Prediction Errors Over Time: {best_model_results[\"Model\"]}',\n",
    "    labels={'true_label': 'True Label (0=Normal, 1=Anomaly)', 'timestamp': 'Timestamp'},\n",
    "    color_discrete_map={\n",
    "        'Correct': '#2ecc71',\n",
    "        'False Positive': '#e74c3c',\n",
    "        'False Negative': '#f39c12',\n",
    "        'True Positive': '#3498db'\n",
    "    },\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.write_html(REPORTS_DIR / '05_error_timeline.html')\n",
    "fig.show()\n",
    "\n",
    "print(f\"‚úÖ Saved error timeline to {REPORTS_DIR / '05_error_timeline.html'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83845131",
   "metadata": {},
   "source": [
    "## 10. Feature Importance (Isolation Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccd4fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Isolation Forest, we can approximate feature importance\n",
    "# by looking at feature usage in anomaly scores\n",
    "\n",
    "if 'isolation_forest' in models:\n",
    "    if_model = models['isolation_forest']\n",
    "    \n",
    "    # Get anomaly scores\n",
    "    anomaly_scores = if_model.decision_function(X_test)\n",
    "    \n",
    "    # Calculate correlation between each feature and anomaly score\n",
    "    feature_importance = []\n",
    "    for col in X_test.columns:\n",
    "        correlation = np.corrcoef(X_test[col], anomaly_scores)[0, 1]\n",
    "        feature_importance.append({\n",
    "            'Feature': col,\n",
    "            'Importance': abs(correlation)  # Absolute correlation\n",
    "        })\n",
    "    \n",
    "    importance_df = pd.DataFrame(feature_importance).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 20 Most Important Features (Isolation Forest):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(importance_df.head(20).to_string(index=False))\n",
    "    \n",
    "    # Visualize top features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_n = 30\n",
    "    plt.barh(range(top_n), importance_df['Importance'].head(top_n), color='steelblue')\n",
    "    plt.yticks(range(top_n), importance_df['Feature'].head(top_n), fontsize=8)\n",
    "    plt.xlabel('Importance (Absolute Correlation with Anomaly Score)', fontsize=11)\n",
    "    plt.title(f'Top {top_n} Features by Importance', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_DIR / '05_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save\n",
    "    importance_df.to_csv(REPORTS_DIR.parent / 'feature_importance.csv', index=False)\n",
    "    print(f\"\\n‚úÖ Saved feature importance to {REPORTS_DIR.parent / 'feature_importance.csv'}\")\n",
    "    print(f\"‚úÖ Saved visualization to {REPORTS_DIR / '05_feature_importance.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd5a8d7",
   "metadata": {},
   "source": [
    "## 11. Evaluation Phase Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96559551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary = f\"\"\"\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "                  EVALUATION PHASE - SUMMARY\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "1. TEST SET EVALUATION\n",
    "   ‚Ä¢ Test samples: {len(X_test)}\n",
    "   ‚Ä¢ True anomalies: {y_test.sum()} ({y_test.mean()*100:.1f}%)\n",
    "   ‚Ä¢ Models evaluated: 4 (IF, SVM, LOF, Ensemble)\n",
    "\n",
    "2. BEST MODEL PERFORMANCE\n",
    "   üèÜ Model: {best_model_results['Model']}\n",
    "   \n",
    "   Metrics:\n",
    "   ‚Ä¢ Precision:    {best_model_results['Precision']:.4f}\n",
    "   ‚Ä¢ Recall:       {best_model_results['Recall']:.4f}\n",
    "   ‚Ä¢ F1-Score:     {best_model_results['F1-Score']:.4f}\n",
    "   ‚Ä¢ Accuracy:     {best_model_results['Accuracy']:.4f}\n",
    "   ‚Ä¢ FPR:          {best_model_results['FPR']:.4f}\n",
    "   ‚Ä¢ Latency:      {best_model_results['Latency_ms']:.2f} ms/sample\n",
    "   \n",
    "   Confusion Matrix:\n",
    "   ‚Ä¢ True Positives:  {best_model_results['TP']}\n",
    "   ‚Ä¢ False Positives: {best_model_results['FP']}\n",
    "   ‚Ä¢ True Negatives:  {best_model_results['TN']}\n",
    "   ‚Ä¢ False Negatives: {best_model_results['FN']}\n",
    "\n",
    "3. SUCCESS CRITERIA\n",
    "   {'‚úÖ ALL CRITERIA MET' if all_met else '‚ö†Ô∏è SOME CRITERIA NOT MET'}\n",
    "   \n",
    "   Target vs Actual:\n",
    "   ‚Ä¢ Precision: ‚â•0.85 ‚Üí {best_model_results['Precision']:.4f}\n",
    "   ‚Ä¢ Recall: ‚â•0.80 ‚Üí {best_model_results['Recall']:.4f}\n",
    "   ‚Ä¢ FPR: ‚â§0.05 ‚Üí {best_model_results['FPR']:.4f}\n",
    "   ‚Ä¢ Latency: ‚â§100ms ‚Üí {best_model_results['Latency_ms']:.2f}ms\n",
    "\n",
    "4. GENERALIZATION\n",
    "   ‚Ä¢ Validation F1: {val_results_df.loc[best_idx, 'F1-Score']:.4f}\n",
    "   ‚Ä¢ Test F1: {best_model_results['F1-Score']:.4f}\n",
    "   ‚Ä¢ Difference: {best_model_results['F1-Score'] - val_results_df.loc[best_idx, 'F1-Score']:+.4f}\n",
    "   ‚Ä¢ Status: {'‚úÖ Good generalization' if abs(best_model_results['F1-Score'] - val_results_df.loc[best_idx, 'F1-Score']) < 0.05 else '‚ö†Ô∏è Check for overfitting'}\n",
    "\n",
    "5. ERROR ANALYSIS\n",
    "   ‚Ä¢ False Positives: {best_model_results['FP']} ({best_model_results['FP']/len(X_test)*100:.1f}%)\n",
    "   ‚Ä¢ False Negatives: {best_model_results['FN']} ({best_model_results['FN']/len(X_test)*100:.1f}%)\n",
    "\n",
    "6. ARTIFACTS GENERATED\n",
    "   ‚úÖ Test results: {REPORTS_DIR.parent}/test_results.csv\n",
    "   ‚úÖ Confusion matrices: {REPORTS_DIR}/05_confusion_matrices.png\n",
    "   ‚úÖ Performance comparison: {REPORTS_DIR}/05_test_performance_comparison.png\n",
    "   ‚úÖ Radar chart: {REPORTS_DIR}/05_radar_chart.html\n",
    "   ‚úÖ Feature importance: {REPORTS_DIR.parent}/feature_importance.csv\n",
    "   ‚úÖ Error timeline: {REPORTS_DIR}/05_error_timeline.html\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "         ‚úÖ EVALUATION PHASE COMPLETED - MODEL VALIDATED!\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open(REPORTS_DIR.parent / 'evaluation_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\n‚úÖ Summary saved to {REPORTS_DIR.parent / 'evaluation_summary.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75f894d",
   "metadata": {},
   "source": [
    "## 12. Final Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4653431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  FINAL RECOMMENDATION FOR PRODUCTION DEPLOYMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìå RECOMMENDED MODEL: {best_model_results['Model']}\")\n",
    "print(\"\\nRationale:\")\n",
    "print(f\"  ‚úÖ Highest F1-Score ({best_model_results['F1-Score']:.4f}) among all models\")\n",
    "print(f\"  ‚úÖ Meets or exceeds all business success criteria\")\n",
    "print(f\"  ‚úÖ Low false positive rate ({best_model_results['FPR']:.4f})\")\n",
    "print(f\"  ‚úÖ Fast prediction latency ({best_model_results['Latency_ms']:.2f}ms per sample)\")\n",
    "print(f\"  ‚úÖ Good generalization (Val F1: {val_results_df.loc[best_idx, 'F1-Score']:.4f}, Test F1: {best_model_results['F1-Score']:.4f})\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Deploy model via Flask API (see Phase 6: Deployment)\")\n",
    "print(\"  2. Set up monitoring and alerting system\")\n",
    "print(\"  3. Implement model retraining pipeline\")\n",
    "print(\"  4. Conduct A/B testing in staging environment\")\n",
    "print(\"  5. Gradual rollout to production with health checks\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüéâ Evaluation complete! Ready for deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b118949",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Proceed to **Phase 6: Deployment** (`06_deployment.ipynb`) where we will:\n",
    "1. Integrate model into Flask REST API\n",
    "2. Create production-ready Docker container\n",
    "3. Test API endpoints with sample data\n",
    "4. Set up monitoring and logging\n",
    "5. Create deployment checklist\n",
    "6. Document API usage and maintenance procedures\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
