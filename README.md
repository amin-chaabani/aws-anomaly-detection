# AWS Anomaly Detection System

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## üìã Overview

An intelligent anomaly detection system for AWS infrastructure monitoring using machine learning. This project implements the CRISP-DM methodology to detect anomalies in cluster metrics (CPU, Memory, and Pod ratios).

## üéØ Project Highlights

- **Final Model**: One-Class SVM with RBF kernel
- **Performance**: F1-Score = 0.625, Precision = 0.714, Recall = 0.556
- **Low False Positive Rate**: 7.7%
- **Fast Inference**: 0.14 ms per sample
- **No Overfitting**: Excellent generalization (Val-Test diff < 0.01)

## üèóÔ∏è Project Structure

```
yassmine/
‚îú‚îÄ‚îÄ aws_anomaly_detection_project/     # Main project directory
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/                     # Jupyter notebooks (CRISP-DM phases)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_business_understanding.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02_data_understanding.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 03_data_preparation.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 04_modeling.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 05_evaluation.ipynb
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 06_deployment.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ src/                          # Source code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py
‚îÇ   ‚îú‚îÄ‚îÄ api/                          # Flask API
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ app.py
‚îÇ   ‚îú‚îÄ‚îÄ data/                         # Data files
‚îÇ   ‚îú‚îÄ‚îÄ models/                       # Trained models
‚îÇ   ‚îî‚îÄ‚îÄ reports/                      # Visualizations and reports
‚îú‚îÄ‚îÄ models/                           # Final models
‚îÇ   ‚îú‚îÄ‚îÄ one_class_svm_final.pkl
‚îÇ   ‚îú‚îÄ‚îÄ final_model_config.pkl
‚îÇ   ‚îú‚îÄ‚îÄ scaler.pkl
‚îÇ   ‚îî‚îÄ‚îÄ feature_names.pkl
‚îú‚îÄ‚îÄ synthetic_anomalies.py            # Anomaly generation utilities
‚îú‚îÄ‚îÄ feature_engineering.py            # Feature engineering module
‚îú‚îÄ‚îÄ requirements.txt                  # Project dependencies
‚îî‚îÄ‚îÄ README.md                         # This file
```

## üöÄ Quick Start

### Prerequisites

- Python 3.11+
- pip
- virtualenv (recommended)

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd yassmine
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   
   # Windows
   venv\Scripts\activate
   
   # Linux/Mac
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

### Usage

#### 1. Run Jupyter Notebooks

```bash
jupyter notebook aws_anomaly_detection_project/notebooks/
```

Navigate through the notebooks in order (01 ‚Üí 06) to see the complete CRISP-DM workflow.

#### 2. Use the Trained Model

```python
import pickle
import pandas as pd

# Load the model
with open('models/one_class_svm_final.pkl', 'rb') as f:
    model = pickle.load(f)

with open('models/scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)

# Prepare your data
X_new = pd.DataFrame(...)  # Your metrics data
X_scaled = scaler.transform(X_new)

# Make predictions
predictions = model.predict(X_scaled)
# -1 = anomaly, 1 = normal
```

#### 3. Run the API (if deployed)

```bash
cd aws_anomaly_detection_project
python api/app.py
```

## üìä Dataset

- **Source**: AWS Prometheus metrics
- **Metrics**:
  - `cluster_cpu_request_ratio`: CPU resource requests vs available
  - `cluster_mem_request_ratio`: Memory resource requests vs available
  - `cluster_pod_ratio`: Pod count vs capacity
- **Samples**: 230 total (161 train, 34 validation, 35 test)
- **Features**: 104 engineered features
- **Anomaly Rate**: ~26% (synthetic anomalies)

## üî¨ Methodology

This project follows the **CRISP-DM** (Cross-Industry Standard Process for Data Mining) methodology:

1. **Business Understanding**: Define objectives and requirements
2. **Data Understanding**: EDA, statistical analysis, outlier detection
3. **Data Preparation**: Feature engineering, scaling, train-test split
4. **Modeling**: Algorithm selection, hyperparameter tuning with Optuna
5. **Evaluation**: Performance metrics, overfitting detection
6. **Deployment**: API development, Docker containerization

## üèÜ Model Performance

### Final Model: One-Class SVM

| Metric | Validation | Test | Difference |
|--------|-----------|------|------------|
| **F1-Score** | 0.632 | 0.625 | 0.007 |
| **Precision** | 0.714 | 0.714 | 0.000 |
| **Recall** | 0.556 | 0.556 | 0.000 |
| **FPR** | 0.077 | 0.077 | 0.000 |

### Confusion Matrix (Test Set)
```
              Predicted
              Normal  Anomaly
Actual Normal    24      2
       Anomaly    4      5
```

## üìà Key Features

- ‚úÖ **Robust Feature Engineering**: 104 features including rolling statistics, lag features, and temporal patterns
- ‚úÖ **Hyperparameter Optimization**: Optuna-based Bayesian optimization (40 trials)
- ‚úÖ **Overfitting Prevention**: Stratified splits, careful validation, model selection based on generalization
- ‚úÖ **Comprehensive Evaluation**: Multiple metrics, confusion matrix, error analysis
- ‚úÖ **Production Ready**: Fast inference, low false positive rate, stable performance

## üìö Documentation

- **[Phase 4 Complete Report](PHASE_4_COMPLETE.md)**: Modeling phase summary
- **[Modeling Lessons Learned](MODELING_LESSONS_LEARNED.md)**: 50+ insights from the modeling process
- **[Ready for Phase 5](READY_FOR_PHASE_5.md)**: Handoff guide for evaluation phase
- **[Project Summary](PROJECT_SUMMARY.md)**: Overall project overview
- **[Enhancement Summary](ENHANCEMENTS_SUMMARY.md)**: Recent improvements

## üõ†Ô∏è Technologies Used

- **Python 3.11**: Core programming language
- **scikit-learn**: Machine learning algorithms
- **Optuna**: Hyperparameter optimization
- **Pandas**: Data manipulation
- **NumPy**: Numerical computing
- **Matplotlib/Seaborn/Plotly**: Data visualization
- **Jupyter**: Interactive notebooks
- **Flask**: API framework (deployment)
- **Docker**: Containerization (deployment)

## üîç Model Selection Process

We evaluated multiple approaches:
- ‚úÖ **One-Class SVM** (Selected): Best generalization, stable performance
- ‚ùå Isolation Forest: Overfitting (Val F1=0.737, Test F1=0.471)
- ‚ùå Local Outlier Factor: Overfitting (Val F1=0.737, Test F1=0.429)
- ‚ùå Feature Selection + SVM: Severe overfitting (Val F1=0.737, Test F1=0.471)

**Selection Criteria**: 
- Val-Test F1 difference < 0.01
- Test F1 > 0.60
- Low false positive rate
- Interpretability

## üöß Future Improvements

- [ ] Real-time monitoring dashboard
- [ ] Additional anomaly types (concept drift, seasonal anomalies)
- [ ] Deep learning approaches (LSTM Autoencoder)
- [ ] Automated retraining pipeline
- [ ] Alert system integration
- [ ] Multi-cluster support

## üë• Contributors

- **Yassmine** - Data Scientist

## üìù License

This project is licensed under the MIT License - see the LICENSE file for details.

## üôè Acknowledgments

- AWS Prometheus for metric collection
- CRISP-DM methodology for structured approach
- scikit-learn and Optuna communities

## üìû Contact

For questions or feedback, please open an issue in the repository.

---

**Last Updated**: November 4, 2025
**Status**: Phase 4 (Modeling) Complete ‚úÖ | Phase 5 (Evaluation) In Progress üîÑ
